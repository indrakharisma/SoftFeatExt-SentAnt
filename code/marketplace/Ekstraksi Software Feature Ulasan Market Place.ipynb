{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import en_core_web_md\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MASUKKAN DATA ULASAN & DATA DAFTAR KEBUTUHAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ulasan = open(\"Ulasan-MarketPlace.txt\", \"r\", encoding='utf-8')\n",
    "print_req = open(\"Req-Comm.txt\", \"r\", encoding='utf-8')\n",
    "ulasan = print_ulasan.read()\n",
    "requirement = print_req.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ulasan)\n",
    "print(ulasan)\n",
    "#x = ulasan.split('.')\n",
    "#print(\"\\n\\n\")\n",
    "#print(x)\n",
    "\n",
    "#print(req)\n",
    "print(requirement)\n",
    "r = requirement.replace('+','\\n')\n",
    "print(\"\\n\\n\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRAPROSES DATA REQUIREMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = r.translate(str.maketrans('', '', string.punctuation))\n",
    "#1.Case Folding\n",
    "req_n = spell(req).lower()\n",
    "#print ('\\n'+req_n)\n",
    "\n",
    "#2.Tokenisasi\n",
    "req_t = req_n.split('\\n')\n",
    "#print (ulasan_t)\n",
    "\n",
    "#3.Hapus Stopwords\n",
    "list_req=[]\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['user']\n",
    "stop_words.extend(newStopWords)\n",
    "for i in range(len(req_t)):\n",
    "    token = nltk.word_tokenize(req_t[i])\n",
    "    filtered_sentence = [w for w in token if not w in stop_words]\n",
    "    filtered_sentence = []\n",
    "\n",
    "    for w in token: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w+\" \")\n",
    "    string_fs = \"\"\n",
    "    string_fs = string_fs.join(filtered_sentence)\n",
    "    list_req.append(string_fs)\n",
    "print(list_req)\n",
    "#print(req_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRAPOSES DATA ULASAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Pembersihan Data\n",
    "import re\n",
    "#hapus kelebihan titik\n",
    "ulasan_c =ulasan.replace(\"....\",\".\")\n",
    "ulasan_c =ulasan_c.replace(\"...\",\".\")\n",
    "ulasan_c =ulasan_c.replace(\"..\",\".\")\n",
    "ulasan_c =ulasan_c.replace('\"',\"\")\n",
    "ulasan_c =ulasan_c.replace(\"\\n\",\" \")\n",
    "ulasan_c =ulasan_c.replace(\"won't\",\"will not\")\n",
    "ulasan_c =ulasan_c.replace(\"app \",\"application \")\n",
    "ulasan_c =ulasan_c.replace(\"n't\",\" not\")\n",
    "ulasan_c =re.sub('(?<=\\d).(?=\\d)', ',', ulasan_c)\n",
    "ulasan_c =ulasan_c.replace(\".\",\". \")\n",
    "print (ulasan_c)\n",
    "\n",
    "#hapus emoji\n",
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')\n",
    "ulasan_c=deEmojify(ulasan_c)\n",
    "#print('ulasan c \\n'+ulasan_c)\n",
    "\n",
    "#2.Spelling Normalization\n",
    "ulasan_n = spell(ulasan_c)\n",
    "print('Spelling Normalization')\n",
    "print ('ulasan n \\n'+ulasan_n)\n",
    "\n",
    "#3.Tokenisasi\n",
    "ulasan_t = sent_tokenize(ulasan_n)\n",
    "print('Tokenisasi')\n",
    "print (ulasan_t)\n",
    "\n",
    "#4.POS Tagging\n",
    "list_ulasan=[]\n",
    "for i in range(len(ulasan_t)):\n",
    "    token = nltk.word_tokenize(ulasan_t[i])\n",
    "    list_ulasan.append(nltk.pos_tag(token))\n",
    "\n",
    "print('\\n+POS Tagging')\n",
    "print(list_ulasan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALISIS SENTIMEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Teks','Polarity','Subjectivity','SF'])\n",
    "for i in range(len(ulasan_t)):\n",
    "    ulasanTB= TextBlob(ulasan_t[i])\n",
    "    polarity = ulasanTB.sentiment.polarity\n",
    "    subjectivity = ulasanTB.sentiment.subjectivity\n",
    "    df.loc[i, ['Teks']] = ulasan_t[i]\n",
    "    df.loc[i, ['Polarity']] = polarity\n",
    "    df.loc[i, ['Subjectivity']] = subjectivity\n",
    "df\n",
    "df.to_excel(r'SENTIMEN-Commerce.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS TAG CHUNKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converttostr(input_seq, seperator):\n",
    "   # Join all the strings in list\n",
    "   final_str = seperator.join(input_seq)\n",
    "   return final_str\n",
    "grammar = ('''PN:{<PRP\\$|PRP>+}\n",
    "NC:{(<DT>*<JJ|JJS|JJR>*)?(<IN>*<NN|NNP|NNS|NNPS>)*} \n",
    "VERB:{<VB|VBG|VBZ|VBD|VBN|VBP>+<IN>*} \n",
    "NP:{((<NC>+<,>)*<NC>*<CC>+)*<NC|IN>+}\n",
    "SF: {(<VERB|(JJ|JJS|JJR)|NP>+<DT>*<NP|(RB|RBR|RBS)|(JJ|JJS|JJR)>+)+}\n",
    "    ''')\n",
    "\n",
    "chunkParser = nltk.RegexpParser(grammar)\n",
    "\n",
    "for i in range(len(list_ulasan)):\n",
    "    chunkList=[]\n",
    "    print(str(i)+'.'+str(ulasan_t[i]))\n",
    "    tree = chunkParser.parse(list_ulasan[i])\n",
    "    #tree.draw()\n",
    "    for j in tree.subtrees(filter=lambda x: x.label() == 'SF'):\n",
    "        print (j)\n",
    "        #print (get_word(j))\n",
    "        text=' '.join([w for w, t in j.leaves()])\n",
    "        chunkList.append(text)\n",
    "    print(chunkList)\n",
    "    df.loc[i, ['SF']] = converttostr(chunkList, ',')\n",
    "\n",
    "df\n",
    "\n",
    "df_new=(df.set_index(['Teks','Polarity','Subjectivity'])\n",
    "   .apply(lambda x: x.str.split(',').explode())\n",
    "   .reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def lemmatize_text(text):\n",
    "    output=\"\"\n",
    "    for sentence in text:\n",
    "        output=(\" \".join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]))\n",
    "    return output\n",
    "\n",
    "def stopwords_removal(text):\n",
    "    filtered_sentence=\"\"\n",
    "    for sentence in text:\n",
    "        filtered_sentence = (\" \".join([w for w in w_tokenizer.tokenize(text) if not w in stop_words]))\n",
    "    return filtered_sentence\n",
    "\n",
    "df_ekstraksi = df_new\n",
    "df_ekstraksi['SF'] = df_ekstraksi['SF'].str.lower()\n",
    "df_ekstraksi['SF'] = df_ekstraksi.SF.apply(lemmatize_text)\n",
    "df_ekstraksi['SF'] = df_ekstraksi.SF.apply(stopwords_removal)\n",
    "df_group=df_ekstraksi\n",
    "df_ekstraksi = df_ekstraksi.groupby('SF').agg({'Polarity':['min','max','mean'],'Subjectivity':['min','max','mean']}).reset_index()\n",
    "\n",
    "group=df_group.groupby(['SF'])['Teks'].count().reset_index()\n",
    "df_ekstraksi['Count'] = group['Teks']\n",
    "df_ekstraksi=df_ekstraksi.sort_values('Count',ascending=False)\n",
    "df_ekstraksi=df_ekstraksi[df_ekstraksi.SF != '']\n",
    "df_ekstraksi['SF'] = df_ekstraksi['SF'].str.replace('^ +| +$','_')\n",
    "df_ekstraksi['SF'] = df_ekstraksi['SF'].str.replace('_','')\n",
    "df_ekstraksi=df_ekstraksi[df_ekstraksi['SF'].str.contains(\" \")]\n",
    "df_ekstraksi=df_ekstraksi[df_ekstraksi['SF'].str.contains(\" \")]\n",
    "df_ekstraksi=df_ekstraksi.reset_index(drop=True)\n",
    "df_ekstraksi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_req)):\n",
    "    df_ekstraksi[str(i+1)] = np.nan\n",
    "    for j in range(len(df_ekstraksi)):\n",
    "        #print('req='+str(i)+\"||\"+'ulasan='+str(j))\n",
    "        position=str(i+1)\n",
    "        token_req=nlp(list_req[i])\n",
    "        token_ulasan=nlp(df_ekstraksi.loc[j,'SF'].to_string())\n",
    "        df_ekstraksi.loc[j, [position]]=token_req.similarity(token_ulasan)\n",
    "df_ekstraksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ekstraksi.to_excel(r'Similarity-MarketPlace-.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILTER AND REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "writer = pd.ExcelWriter('SF-MarketPlace-New.xlsx',engine='xlsxwriter')\n",
    "\n",
    "cell_format = workbook.add_format({'align': 'center',\n",
    "                                   'valign': 'vcenter',\n",
    "                                   'bold': True})\n",
    "for i in range(len(list_req)):\n",
    "    position=str(i+1)\n",
    "    print(position)\n",
    "    df_filter = df_ekstraksi[df_ekstraksi[str(i+1)]>=0.5]\n",
    "    df_filter= df_filter[['SF','Polarity','Subjectivity','Count',position]].sort_values(by=position,ascending=False)\n",
    "    df_filter.rename(columns={position: 'Similarity'}, inplace=True)\n",
    "    df_filter= df_filter.reset_index(drop=True)\n",
    "    df_filter.set_index(['SF'])\n",
    "    df_filter.to_excel(writer, sheet_name=position, startrow=1, index=True) \n",
    "    worksheet = writer.sheets[position]\n",
    "    worksheet.merge_range('B1:J1',str(req_t[i].upper()),cell_format)\n",
    "    #worksheet.write_string(0, 1, str(req_t[i]))\n",
    "    #worksheet.write_string('A1', str(req_t[i]))\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
